> Este material se basa en [Clustering con Python by JoaquÃ­n Amat Rodrigo](https://www.cienciadedatos.net/documentos/py20-clustering-con-python.html) 

# Pasos previos
Para este recorrido necesitarÃ¡s las librerÃ­as [Pandas](https://pandas.pydata.org/), [Seaborn](https://seaborn.pydata.org/) y [Scikitlearn](https://scikit-learn.org/stable/index.html)

Podes corroborar si las tienes instaladas corriendo las siguientes lÃ­neas en tu intÃ©rprete de Python:

```python
import pandas as pd
import seaborn as sns
import sklearn
```

Si correr estas lineas no tira ningÃºn error, etonces estÃ¡n felizmente instaladas las bibliotecas enc uestiÃ³n. De lo contrario, obtendremos un mensaje de error `ModuleNotFoundError: No module named` al correr las lineas anteriores. En tal caso, podÃ©s instalar las bibliotecas desde la consola, con el comando:

```bash
        pip install pandas
        pip install seaborn
        pip install sklearn
```


# Guias de Trabajo
 * [1. Clustering Â¿QuÃ© es?](#1-Intro)
 * [2. Un ojo en el Iris](#1-Iris)
 * [3. Calculo de distancias](#3-distancia)
 * [4. Normalizado y escalado de los datos](#4-escalado)
 * [5. K-means](#5-kmeans)
 * [6. EvaluaciÃ³n del resultado obtenido](#6-inercia)

[1. Clustering Â¿QuÃ© es?](#1-Intro)


Hemos estado trabajando hasta aquÃ­ en la carga y limpieza da datos con Pandas. Es momento de comenzar a trabajar con los datos, analizarlos y poder encontrar patrones que nos permitan derivar informaciÃ³n. El aprendizaje automÃ¡tico consiste en identificar de patrones o tendencias que de los datos de forma automÃ¡tica.

> Para pensar ğŸ¤”: Â¿QuÃ© utilidad le encontrÃ¡s al aprendizaje automatizado? Â¿QuÃ© aplicaciones se te ocurren o conoces?


<details>
  <summary>Recurso audiovisuales complementarios</summary>

[Inteligencia Artificial: Â¿Amiga o Enemiga? | Diego FernÃ¡ndez Slezak](https://www.youtube.com/watch?v=znq3ql6wqnE)

[Â¿De quÃ© es capaz la inteligencia artificial? | DW Documental](https://www.youtube.com/watch?v=34Kz-PP_X7c&t=146s)
</details>

En este recorrido nos centraremos particularmente en mÃ©todos que nos permitan describir la estructura u organizaciÃ³n de nuestros datos. Los mÃ©todos de clustering(agrupamientos) nos permiten buscar patrones "ocultos" en los datos sin la necesidad de contar con la informaciÃ³n sobre la verdadera clasificaciÃ³n (etiquetas) de los datos. Son mÃ©todos exploratorios que agrupan los datos segÃºn similitud para simplificar su anÃ¡lisis.

> Para pensar ğŸ¤”: Â¿QuÃ© tipo de simplificaciones permite el agrupamiento de datos?

Existen una amplia cantidad de tÃ©cnicas que nos permiten encontrar patrones en los datos y agruparlas de algÃºn modo, pero en todos los casos estos agrupamientos se establecen de forma que, las observaciones que estÃ¡n dentro de un mismo grupo, son similares entre ellas y distintas a las de otros grupos. Todos los mÃ©todos de clustering requieren de la definiciÃ³n y cuantificaciÃ³n de la similitud entre las observaciones. Es por ello que resulta necesario revisar el concepto de distancia, ya que es lo que se usa como medida de similitud o diferencia entre grupos.



[2. Un ojo en el Iris](#2-Iris)


En este recorrido utilizaremos el [set de datos](https://github.com/flbulgarelli/recursos-python/blob/master/2_Ciencia_de_datos_pandas/iris_data.txt) [Iris](https://en.wikipedia.org/wiki/Iris_flower_data_set) que consiste en un conjunto de datos u observaciones realizadas por el biÃ³logo Ronald Fisher, sobre las caracterÃ­stica de distintas especies de plantas plantas. Â¿SerÃ¡ posible clasificar las plantas utilizando alguno de estas observaciones que hizo Fisher?

Vamos a explorar los datos:

```python
import pandas as pd

iris = pd.read_csv(datapath + "iris/iris_hidden.txt", sep = '\t')
```


>  ğŸ§—â€â™€ï¸ DesafÃ­o I: AveriguÃ¡ quÃ© variables (columnas) tiene la tabla e inspecionÃ¡ el DataFrame

Para tener una idea mÃ¡s intuitiva del "comportamiento" de los datos podemos graficar la distribuciÃ³n de frecuencias de las distintas variables que nos permitirÃ¡, por ejemplo,  saber si las observaciones son Ãºnicas o se repiten. Para ello utilizaremos la biblioteca [Seaborn](https://seaborn.pydata.org/):

```python
import seaborn as sns

g = sns.histplot(data = iris, x = "sepal.length", binwidth=0.25, kde = True)

```


> Para pensar ğŸ¤”: Â¿QuÃ© informaciÃ³n obtenes del grÃ¡fico? 
>
> ğŸ§—â€â™€ï¸ DesafÃ­o II: Grafica la distribuciÃ³n de frecuencias de la variable "petal.length" Â¿QuÃ© informaciÃ³n obtenes del grÃ¡fico? Â¿QuÃ© diferencias notÃ¡s respecto del observado para la variable sepal.length? 
>
> ğŸ§—â€â™€ï¸ DesafÃ­o III: Grafica la distribuciÃ³n de frecuencias del resto de las variables.
>
> Para pensar ğŸ¤”: Â¿QuÃ© informaciÃ³n pudiste obtener de observar las distribuciones de las distintas variables? Â¿CuÃ¡ntos tipos de plantas crees que existen?

Ahora que pudimos observar como se comportan las variables, nos puede ser de gran utilidad estudiar las asociaciones entre las mismas (correlaciÃ³n). De este modo sabremos si el comportamiento (crecimiento o disminuciÃ³n) de una variable, se debe o estÃ¡ influenciada por otra. Con los pairplots de seabron, podemos entonces estudiar si existen correlaciones entre las variables:


```python
import seaborn as sns

g = sns.pairplot(iris)

```
> ğŸ§—â€â™€ï¸ DesafÃ­o IV: Â¿Existe alguna correlaciÃ³n entre algunas de las variables? Â¿CÃ³mo te diste cuenta? 



[3. Calculo de distancias](#3-distancia)

Hemos observado las distribuciones de nuestros datos y la manera en que se correlacionan las variables, y de este modo comenzar a intuir posibles agrupamientos de los datos. Es decir, pudimos observar mediante grÃ¡ficos exploratorios que algunos registros muestran una mayor similitud entre si.

Justamente, los mÃ©todos de clustering permiten la identificaciÃ³n automÃ¡tica de grupos en los que se pueden agrupar las observaciones de un conjunto de datos. Esto se hace de forma tal que las observaciones o registros asignados a un mismo grupo, muestren una mayor similitud entre sÃ­ que con los miembros de otros grupos. Pero, Â¿CÃ³mo medimos similitud entre miembros de un grupo dado? ğŸ¤”


Hagamos un prueba para intentar dar respuesta a esta pregunta. Tomemos, por ejemplo, un conjunto de emojis ğŸ™€ğŸ˜»ğŸ¥°ğŸ˜±ğŸ˜¾ğŸ™ŠğŸ™ˆğŸ˜ 
> Para pensar ğŸ¤”: Â¿CÃ³mo los agruparÃ­as/clasificarÃ­as?Â¿Se te ocurre mÃ¡s de una forma de clasificarlos?Â¿QuÃ© criterios usaste para cada caso?

Como habrÃ¡s observado, la clasificaciÃ³n de los datos es subjetiva, en tanto depende del modo en que decidimos medir la similitud entre las observaciones. Y tal como hemos visto, existen mÃºltiples formas de calcular lasimilitud entre los datos. 

Una forma de obtener la similitud es asumir que los datos son puntos en el espacio, por lo que si se define la distancia ente los puntos y se mide la separaciÃ³n entre dos registros, podrÃ¡ obtenerse la similitud entre estos. 

Una de las formas mÃ¡s bÃ¡sicas para calcular la **distancia**  entre dos puntos es la **EuclÃ­dea**, basada en el famoso [Teorema de PitÃ¡goras](https://es.wikipedia.org/wiki/Teorema_de_Pit%C3%A1goras).. Â¡Si, era importante PitÃ¡goras!

![Distancia Euclidea](./dist_euclÃ­dea.gif)


Â¿Pero no todas las definiciones de distancia son aplicables a todos los tipos de datos no? Â¡Claro que no! Â¿Como por ejemplo...? ğŸ¤”


> Para pensar ğŸ¤”: Â¿QuÃ© otras formas de caluclar la distancia se te ocurren?
>
> ğŸ§—â€â™€ï¸ DesafÃ­o V: BuscÃ¡ otras formas de calcular la distancia entre las observaciones Â¿QuÃ© ventajas o desventajas encontras en cada forma de calcular las distancias?

[4. Normalizado y escalado de los datos](#4-escalado)

Ya hemos identificado las problemÃ¡ticas a la hora de clasificar los datos, pero para que las comparaciones que hagamos sean completamente vÃ¡lidas, resulta de suma importancia hacer un tratamiento extra de los datos. 

Uno de los tratamientos necesarios es el escalado de los datos. Este procedimiento nos permite asegurarnos de que aÃºn cuando algunas variables toman valores mÃ¡s grandes no se usarÃ¡n como predictor principal a la hora de clasificar.


Veamos un ejemplo grÃ¡fico de esta problemÃ¡tica que estamos describiendo: 

![Carrera profesional](./carrera.jpeg)


Imaginemos que tenemos que analizar la trayectoria profesional de dos personas, para hacer una selecciÃ³n laboral. A priori, serÃ­a lÃ³gico pensar en basar esta selecciÃ³n en el curriculum de dichas personas. Sin embargo, resulta evidente que el curriculum no nos da un panaroma completo de las habilidades de una persona. Por ejemplo, no nos permite conocer su capacidad de trabajo en equipo o sus habilidades para realizar mÃ¡s de una tarea a la vez, etc. Â¿QuÃ© peso le estamos dando entonces a estas otras caracterÃ­sticas? Â¿Estamos subvalorando o sobrevalorando las hablidades de las personas? Â¿Que pasa con las personas no tienen las mismas posibilidades para completar su curriculum? Â¿Las hace menos capaces para el trabajo?

Es por ello que resulta necesario escalar los datos. La escala es importante para poder especificar que una modificaciÃ³n en una cantidad no es igual a otra modificaciÃ³n en otra. En pocas palabras, escalar los datos le da a todas las caracterÃ­sticas la misma importancia para que ninguna estÃ© dominada por otra. 

Ademas, resulta necesario antes de comenzar a clasificar nuestros datos es la normalizaciÃ³n. Esta implica transformar o convertir el conjunto de datos en una distribuciÃ³n normal, de forma que todos datos tenga una varianza del mismo orden. De este modo, cada dato nos darÃ¡ una idea de a cuÃ¡ntos desvÃ­os de la media estÃ¡ ese punto.

Estas operaciones pueden hacerse muy fÃ¡cilmente con la clase `StandardScaler`, del mÃ³dulo `scikitlearn`:


```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
iris_escaleado = scaler.fit_transform(iris)
```

[5. K-means](#5-kmeans)

Ahora que hemos normalizado y escalado nuestros datos podemos finalmente utilizar un mÃ©todo para agrupar nuestros datos. Vamos a utilizar el mÃ©todo K-means(MacQueen, 1967) que agrupa las observaciones en los mejores K grupos distintos, es decirlos k clusters con la menor varianza interna (intra-cluster variation) posible. Es decir que se reparten las observaciones en K clusters de forma que la suma de las varianzas internas de todos ellos sea lo menor posible. 

PodrÃ­amos decir que este mÃ©todo repite/itera una serie de pasos hasta que encuentra los mejores k grupos:

![Carrera profesional](./k_means.png)

  1) Especifica el nÃºmero K de clusters que se quieren crear

  2) Selecciona de forma aleatoria k observaciones del set de datos como centroides iniciales, esto es los datos a los cuÃ¡les se calcula la distancia para delimitar el grupo de menor varianza interna

  3) Calcula las distancia de todos los datos al centroide, para definir a cuÃ¡l se encuentra mÃ¡s prÃ³ximo

  4) Para cada uno de los K clusters recalcular su centroide, la posiciÃ³n del centroide se actualiza tomando como nuevo centroide la posiciÃ³n del promedio de las observaciones pertenecientes a dicho grupo

  5) Repete los pasos 3 y 4 hasta que los centroides no se mueven, o se mueven por debajo de una distancia umbral en cada paso, o se alcancen el nÃºmero de iteraciones definidas de antemano.

Apliquemos ahora este mÃ©todo a nuestros datos:


```python
k = 3  #definimos la cantidad de clusters
kmeans = KMeans(n_clusters = k, init="random", n_init=10, max_iter=300, random_state=123457) #tomamos los centroides de forma aleatoria y definimos un mÃ¡ximo de 300 iteraciones
kmeans.fit(iris_escaleado)  #aplicamos el mÃ©todo a nuestros datos
```

La asignaciÃ³n de cada punto a un cluster se obtiene el atributo `labels_` del objeto `clusters`, esta propiedad me dice que etiqueta le puso a cada uno de mis datos. 


```python
print(kmeans.labels_)

```

Los centroides pueden ser obtenidos con `cluster_centers_`:

```python
print(kmeans.cluster_centers_ )

```

Para entender mejor los resultados obtenidos grafiquemos la distribuciÃ³n de puntos, pintando cada punto segÃºn el color correspondiente al etiquetado:


```python
colores = ["red", "green", "blue"]
g = sns.scatterplot(x = iris_escaleado[:,2], y = iris_escaleado[:, 3], hue = kmeans.labels_, palette = colores, alpha = 0.5)
g = sns.scatterplot(x = kmeans.cluster_centers_[:,2], y = kmeans.cluster_centers_[:,3], zorder = 10, palette = colores, hue = [0, 1, 2], legend = False, marker=6, s=200)
```

> Para pensar ğŸ¤”: Â¿Es bueno o malo este resultado? Â¿CÃ³mo podrÃ­amos evaluar el resultado?
>


[6. EvaluaciÃ³n del resultado obtenido](#6-inercia)

 Una forma de evaluar cuan bien funcionÃ³ nuestro agrupamiento podrÃ­a ser calcular cuan compactos son los grupos obtenidos. Entendiendo que funcionÃ³ mejor si todos los elementos del grupo estÃ¡n lo mÃ¡s cerca posible de su centro. Podemos, entonces, sumar las distancias de cada punto a su respectivo centro y usar eso como medida. A este valor se lo denomina inercia y puede obtenerse haciendo:


```python
print(kmeans.inertia_ )

```


>
> ğŸ§—â€â™€ï¸ DesafÃ­o VI: CalculÃ¡ la inercia para distintos valores de k y almacenalos en un DataFrame
>
> ğŸ§—â€â™€ï¸ DesafÃ­o VII: RealizÃ¡ un grÃ¡fico de inercia vs k, usando el mÃ©todo pointplot de seaborn
>


<details>
  <summary>Pista</summary>
sns.pointplot(data = df, x = "K", y = "SSE")
</details>

