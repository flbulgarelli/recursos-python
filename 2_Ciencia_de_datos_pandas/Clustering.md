> Este material se basa en [Clustering con Python by Joaqu√≠n Amat Rodrigo](https://www.cienciadedatos.net/documentos/py20-clustering-con-python.html) 

# Pasos previos
Para este recorrido necesitar√°s las librer√≠as [Pandas](https://pandas.pydata.org/), [Seaborn](https://seaborn.pydata.org/) y [Scipy](https://www.scipy.org/)

Podes corroborar si las tienes instaladas corriendo las siguientes l√≠neas en tu int√©rprete de Python:

```python
import pandas as pd
import seaborn as sns
import scipy.stats as ss
```

Si correr estas lineas no tira ning√∫n error, etonces est√°n felizmente instaladas las bibliotecas enc uesti√≥n. De lo contrario, obtendremos un mensaje de error `ModuleNotFoundError: No module named` al correr las lineas anteriores. En tal caso, pod√©s instalar las bibliotecas desde la consola, con el comando:

```bash
        pip install pandas
        pip install seaborn
        pip install scipy
```


# Guias de Trabajo
 * [1.Clustering ¬øQu√© es?](#1-Intro)
 * [2.Un ojo en el Iris](#1-Iris)
 * [3.Calculo de distancias](#3-distancia)
 * [4.Normalizado y escalado de los datos](#4-escalado)
 * [5.K-means](#5-kmeans)
 * [6.Agrupamiento jer√°rquico](#6-agrupamiento)

[1.Clustering ¬øQu√© es?](#1-Intro)
Hemos estado trabajando hasta aqu√≠ en la carga y limpieza da datos con Pandas. Es momento de comenzar a trabajar con los datos, analizarlos y poder encontrar patrones que nos permitan derivar informaci√≥n. El aprendizaje autom√°tico consiste en identificar de patrones o tendencias que de los datos de forma autom√°tica.

> Para pensar ü§î: ¬øQu√© utilidad le encontr√°s al aprendizaje automatizado? ¬øQu√© aplicaciones se te ocurren o conoces?


<details>
  <summary>Recurso audiovisuales complementarios</summary>

[Inteligencia Artificial: ¬øAmiga o Enemiga? | Diego Fern√°ndez Slezak](https://www.youtube.com/watch?v=znq3ql6wqnE)

[¬øDe qu√© es capaz la inteligencia artificial? | DW Documental](https://www.youtube.com/watch?v=34Kz-PP_X7c&t=146s)
</details>

En este recorrido nos centraremos particularmente en m√©todos que nos permitan describir la estructura u organizaci√≥n de nuestros datos. Los m√©todos de clustering(agrupamientos) nos permiten buscar patrones "ocultos" en los datos sin la necesidad de contar con la informaci√≥n sobre la verdadera clasificaci√≥n (etiquetas) de los datos. Son m√©todos exploratorios que agrupan los datos seg√∫n similitud para simplificar su an√°lisis.

> Para pensar ü§î: ¬øQu√© tipo de simplificaciones permite el agrupamiento de datos?

Existen una amplia cantidad de t√©cnicas que nos permiten encontrar patrones en los datos y agruparlas de alg√∫n modo, pero en todos los casos estos agrupamientos se establecen de forma que, las observaciones que est√°n dentro de un mismo grupo, son similares entre ellas y distintas a las de otros grupos. Todos los m√©todos de clustering requieren de la definici√≥n y cuantificaci√≥n de la similitud entre las observaciones. Es por ello que resulta necesario revisar el concepto de distancia, ya que es lo que se usa como medida de similitud o diferencia entre grupos.



[2.Un ojo en el Iris](#2-Iris)


En este recorrido utilizaremos el [set de datos](https://github.com/flbulgarelli/recursos-python/blob/master/2_Ciencia_de_datos_pandas/iris_data.txt) [Iris](https://en.wikipedia.org/wiki/Iris_flower_data_set) que consiste en un conjunto de datos u observaciones realizadas por el bi√≥logo Ronald Fisher, sobre las caracter√≠stica de distintas especies de plantas plantas. ¬øSer√° posible clasificar las plantas utilizando alguno de estas observaciones que hizo Fisher?

Vamos a explorar los datos:

```python
import pandas as pd

iris = pd.read_csv(datapath + "iris/iris_hidden.txt", sep = '\t')
```


>  üßó‚Äç‚ôÄÔ∏è Desaf√≠o I: Averigu√° qu√© variables (columnas) tiene la tabla e inspecion√° el DataFrame

Para tener una idea m√°s intuitiva del "comportamiento" de los datos podemos graficar la distribuci√≥n de frecuencias de las distintas variables que nos permitir√°, por ejemplo,  saber si las observaciones son √∫nicas o se repiten. Para ello utilizaremos la biblioteca [Seaborn](https://seaborn.pydata.org/):

```python
import seaborn as sns

g = sns.histplot(data = iris, x = "sepal.length", binwidth=0.25, kde = True)

```


> Para pensar ü§î: ¬øQu√© informaci√≥n obtenes del gr√°fico? 
> üßó‚Äç‚ôÄÔ∏è Desaf√≠o II: Grafica la distribuci√≥n de frecuencias de la variable "petal.length" ¬øQu√© informaci√≥n obtenes del gr√°fico? ¬øQu√© diferencias not√°s respecto del observado para la variable sepal.length? 
> üßó‚Äç‚ôÄÔ∏è Desaf√≠o III: Grafica la distribuci√≥n de frecuencias del resto de las variables.
> Para pensar ü§î: ¬øQu√© informaci√≥n pudiste obtener de observar las distribuciones de las distintas variables? ¬øCu√°ntos tipos de plantas crees que existen?

Ahora que pudimos observar como se comportan las variables, nos puede ser de gran utilidad estudiar las asociaciones entre las mismas (correlaci√≥n). De este modo sabremos si el comportamiento (crecimiento o disminuci√≥n) de una variable, se debe o est√° influenciada por otra. Con los pairplots de seabron, podemos entonces estudiar si existen correlaciones entre las variables:


```python
import seaborn as sns

g = sns.pairplot(iris)

```
> üßó‚Äç‚ôÄÔ∏è Desaf√≠o IV: ¬øExiste alguna correlaci√≥n entre algunas de las variables? ¬øC√≥mo te diste cuenta? 



[3.Calculo de distancias](#3-distancia)

Hemos observado las distribuciones de nuestros datos y la manera en que se correlacionan las variables, y de este modo comenzar a intuir posibles agrupamientos de los datos. Es decir, pudimos observar mediante gr√°ficos exploratorios que algunos registros muestran una mayor similitud entre si.

Justamente, los m√©todos de clustering permiten la identificaci√≥n autom√°tica de grupos en los que se pueden agrupar las observaciones de un conjunto de datos. Esto se hace de forma tal que las observaciones o registros asignados a un mismo grupo, muestren una mayor similitud entre s√≠ que con los miembros de otros grupos. Pero, ¬øC√≥mo medimos similitud entre miembros de un grupo dado? ü§î


Hagamos un prueba para intentar dar respuesta a esta pregunta. Tomemos, por ejemplo, un conjunto de emojis üôÄüòªü•∞üò±üòæüôäüôàüò†
> Para pensar ü§î: ¬øC√≥mo los agrupar√≠as/clasificar√≠as?¬øSe te ocurre m√°s de una forma de clasificarlos?¬øQu√© criterios usaste para cada caso?

Como habr√°s observado, la clasificaci√≥n de los datos es subjetiva, en tanto depende del modo en que decidimos medir la similitud entre las observaciones. Y tal como hemos visto, existen m√∫ltiples formas de calcular lasimilitud entre los datos. 

Una forma de obtener la similitud es asumir que los datos son puntos en el espacio, por lo que si se define la distancia ente los puntos y se mide la separaci√≥n entre dos registros, podr√° obtenerse la similitud entre estos. 

Una de las formas m√°s b√°sicas para calcular la **distancia**  entre dos puntos es la **Eucl√≠dea**, basada en el famoso [Teorema de Pit√°goras](https://es.wikipedia.org/wiki/Teorema_de_Pit%C3%A1goras).. ¬°Si, era importante Pit√°goras!

![Distancia Euclidea](./dist_eucl√≠dea.gif)


¬øPero no todas las definiciones de distancia son aplicables a todos los tipos de datos no? ¬°Claro que no! ¬øComo por ejemplo...? ü§î


> Para pensar ü§î: ¬øQu√© otras formas de caluclar la distancia se te ocurren?
>
> üßó‚Äç‚ôÄÔ∏è Desaf√≠o V: Busc√° otras formas de calcular la distancia entre las observaciones ¬øQu√© ventajas o desventajas encontras en cada forma de calcular las distancias?

[4.Normalizado y escalado de los datos](#4-escalado)

Ya hemos identificado las problem√°ticas a la hora de clasificar los datos, pero para que las comparaciones que hagamos sean completamente v√°lidas, resulta de suma importancia hacer un tratamiento extra de los datos. 

Uno de los tratamientos necesarios es el escalado de los datos. Este procedimiento nos permite asegurarnos de que a√∫n cuando algunas variables toman valores m√°s grandes no se usar√°n como predictor principal a la hora de clasificar.


Veamos un ejemplo gr√°fico de esta problem√°tica que estamos describiendo: 

![Carrera profesional](./carrera.jpeg)


Imaginemos que tenemos que analizar la trayectoria profesional de dos personas, para hacer una selecci√≥n laboral. A priori, ser√≠a l√≥gico pensar en basar esta selecci√≥n en el curriculum de dichas personas. Sin embargo, resulta evidente que el curriculum no nos da un panaroma completo de las habilidades de una persona. Por ejemplo, no nos permite conocer su capacidad de trabajo en equipo o sus habilidades para realizar m√°s de una tarea a la vez, o... ¬øQu√© peso le estamos dando entonces a estas otras caracter√≠sticas? ¬øEstamos subvalorando o sobrevalorando hablidades?

Es por ello que resulta necesario escalar los datos. La escala es importante para poder especificar que una modificaci√≥n en una cantidad no es igual a otra modificaci√≥n en otra. En pocas palabras, escalar los datos le da a todas las caracter√≠sticas la misma importancia para que ninguna est√© dominada por otra. 

Otro tratamiento de los datos necesario antes de comenzar a clasificar nuestros datos es la normalizaci√≥n. Esta implica transformar o convertir el conjunto de datos en una distribuci√≥n normal. Algunos algoritmos como M√°quinas Vectores de Soporte convergen mucho m√°s r√°pido en los datos normalizados, por lo que tiene sentido normalizar los datos para obtener mejores resultados.
